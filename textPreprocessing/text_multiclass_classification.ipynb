{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try first with following simplest set of data which are dummy movie \n",
    "# review comments and clasification whether its a good movie or not\n",
    "# taken from https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras/\n",
    "\n",
    "corpus = [\n",
    "    # Positive Reviews\n",
    "\n",
    "    'This is an excellent movie',\n",
    "    'The move was fantastic I like it',\n",
    "    'You should watch it is brilliant',\n",
    "    'Exceptionally good',\n",
    "    'Wonderfully directed and executed I like it',\n",
    "    'Its a fantastic series',\n",
    "    'Never watched such a brillent movie',\n",
    "    'It is a Wonderful movie',\n",
    "\n",
    "    # Negtive Reviews\n",
    "\n",
    "    \"horrible acting\",\n",
    "    'waste of money',\n",
    "    'pathetic picture',\n",
    "    'It was very boring',\n",
    "    'I did not like the movie',\n",
    "    'The movie was horrible',\n",
    "    'I will not recommend',\n",
    "    'The acting is pathetic',\n",
    "    \n",
    "    # average Reviews\n",
    "\n",
    "    \"it is ok\",\n",
    "    'one time watch',\n",
    "    'ok ok types',\n",
    "    'can watch one time',\n",
    "    'not too great'\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = np.array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,2,2,2,2,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "word_tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(word_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_sentences = word_tokenizer.texts_to_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18, 2, 19, 20, 3],\n",
       " [4, 21, 6, 12, 5, 7, 1],\n",
       " [22, 23, 8, 1, 2, 24],\n",
       " [25, 26],\n",
       " [27, 28, 29, 30, 5, 7, 1],\n",
       " [31, 9, 12, 32],\n",
       " [33, 34, 35, 9, 36, 3],\n",
       " [1, 2, 9, 37, 3],\n",
       " [13, 14],\n",
       " [38, 39, 40],\n",
       " [15, 41],\n",
       " [1, 6, 42, 43],\n",
       " [5, 44, 10, 7, 4, 3],\n",
       " [4, 3, 6, 13],\n",
       " [5, 45, 10, 46],\n",
       " [4, 14, 2, 15],\n",
       " [1, 2, 11],\n",
       " [16, 17, 8],\n",
       " [11, 11, 47],\n",
       " [48, 8, 16, 17],\n",
       " [10, 49, 50]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 21, 6, 12, 5, 7, 1] 7\n"
     ]
    }
   ],
   "source": [
    "# Now let's find out what's max length sentence\n",
    "maxiumum = 0\n",
    "word_count = lambda sentence : len(sentence) \n",
    "\n",
    "longest_seq = max(embedded_sentences,key=word_count) \n",
    "longest_seq_length = len(longest_seq)\n",
    "print(longest_seq,longest_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  2 19 20  3  0  0]\n",
      " [ 4 21  6 12  5  7  1]\n",
      " [22 23  8  1  2 24  0]\n",
      " [25 26  0  0  0  0  0]\n",
      " [27 28 29 30  5  7  1]\n",
      " [31  9 12 32  0  0  0]\n",
      " [33 34 35  9 36  3  0]\n",
      " [ 1  2  9 37  3  0  0]\n",
      " [13 14  0  0  0  0  0]\n",
      " [38 39 40  0  0  0  0]\n",
      " [15 41  0  0  0  0  0]\n",
      " [ 1  6 42 43  0  0  0]\n",
      " [ 5 44 10  7  4  3  0]\n",
      " [ 4  3  6 13  0  0  0]\n",
      " [ 5 45 10 46  0  0  0]\n",
      " [ 4 14  2 15  0  0  0]\n",
      " [ 1  2 11  0  0  0  0]\n",
      " [16 17  8  0  0  0  0]\n",
      " [11 11 47  0  0  0  0]\n",
      " [48  8 16 17  0  0  0]\n",
      " [10 49 50  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "pad_sequences = keras.preprocessing.sequence.pad_sequences\n",
    "padded_seq = pad_sequences(embedded_sentences,longest_seq_length,padding=\"post\")\n",
    "print(padded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Now X data is ready to use ... let's one hot encode Y data\n",
    "y = keras.utils.to_categorical(sentiments)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/digishgabhawala/Library/Python/3.5/lib/python/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 7, 20)             1020      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 140)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 423       \n",
      "=================================================================\n",
      "Total params: 1,443\n",
      "Trainable params: 1,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_length,20,input_length=longest_seq_length))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(3,activation='sigmoid'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/digishgabhawala/Library/Python/3.5/lib/python/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 0.6896 - accuracy: 0.6032\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - 0s 74us/step - loss: 0.6868 - accuracy: 0.6984\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - 0s 69us/step - loss: 0.6840 - accuracy: 0.7143\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - 0s 73us/step - loss: 0.6811 - accuracy: 0.7302\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - 0s 78us/step - loss: 0.6783 - accuracy: 0.7619\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - 0s 71us/step - loss: 0.6755 - accuracy: 0.8095\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - 0s 66us/step - loss: 0.6727 - accuracy: 0.8413\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - 0s 67us/step - loss: 0.6699 - accuracy: 0.8571\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - 0s 63us/step - loss: 0.6670 - accuracy: 0.8889\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - 0s 90us/step - loss: 0.6642 - accuracy: 0.9048\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - 0s 80us/step - loss: 0.6614 - accuracy: 0.8889\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - 0s 81us/step - loss: 0.6585 - accuracy: 0.9206\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - 0s 74us/step - loss: 0.6556 - accuracy: 0.9206\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - 0s 77us/step - loss: 0.6528 - accuracy: 0.9048\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - 0s 87us/step - loss: 0.6499 - accuracy: 0.9048\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - 0s 63us/step - loss: 0.6470 - accuracy: 0.9206\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - 0s 78us/step - loss: 0.6440 - accuracy: 0.9206\n",
      "Epoch 18/100\n",
      "21/21 [==============================] - 0s 76us/step - loss: 0.6411 - accuracy: 0.9365\n",
      "Epoch 19/100\n",
      "21/21 [==============================] - 0s 133us/step - loss: 0.6381 - accuracy: 0.9365\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - 0s 97us/step - loss: 0.6351 - accuracy: 0.9365\n",
      "Epoch 21/100\n",
      "21/21 [==============================] - 0s 69us/step - loss: 0.6321 - accuracy: 0.9365\n",
      "Epoch 22/100\n",
      "21/21 [==============================] - 0s 91us/step - loss: 0.6291 - accuracy: 0.9365\n",
      "Epoch 23/100\n",
      "21/21 [==============================] - 0s 133us/step - loss: 0.6260 - accuracy: 0.9365\n",
      "Epoch 24/100\n",
      "21/21 [==============================] - 0s 102us/step - loss: 0.6229 - accuracy: 0.9365\n",
      "Epoch 25/100\n",
      "21/21 [==============================] - 0s 73us/step - loss: 0.6198 - accuracy: 0.9365\n",
      "Epoch 26/100\n",
      "21/21 [==============================] - 0s 86us/step - loss: 0.6166 - accuracy: 0.9365\n",
      "Epoch 27/100\n",
      "21/21 [==============================] - 0s 101us/step - loss: 0.6135 - accuracy: 0.9365\n",
      "Epoch 28/100\n",
      "21/21 [==============================] - 0s 79us/step - loss: 0.6103 - accuracy: 0.9365\n",
      "Epoch 29/100\n",
      "21/21 [==============================] - 0s 64us/step - loss: 0.6070 - accuracy: 0.9365\n",
      "Epoch 30/100\n",
      "21/21 [==============================] - 0s 81us/step - loss: 0.6038 - accuracy: 0.9365\n",
      "Epoch 31/100\n",
      "21/21 [==============================] - 0s 86us/step - loss: 0.6005 - accuracy: 0.9365\n",
      "Epoch 32/100\n",
      "21/21 [==============================] - 0s 72us/step - loss: 0.5971 - accuracy: 0.9365\n",
      "Epoch 33/100\n",
      "21/21 [==============================] - 0s 78us/step - loss: 0.5938 - accuracy: 0.9365\n",
      "Epoch 34/100\n",
      "21/21 [==============================] - 0s 74us/step - loss: 0.5904 - accuracy: 0.9365\n",
      "Epoch 35/100\n",
      "21/21 [==============================] - 0s 64us/step - loss: 0.5870 - accuracy: 0.9365\n",
      "Epoch 36/100\n",
      "21/21 [==============================] - 0s 81us/step - loss: 0.5835 - accuracy: 0.9365\n",
      "Epoch 37/100\n",
      "21/21 [==============================] - 0s 71us/step - loss: 0.5800 - accuracy: 0.9365\n",
      "Epoch 38/100\n",
      "21/21 [==============================] - 0s 98us/step - loss: 0.5765 - accuracy: 0.9524\n",
      "Epoch 39/100\n",
      "21/21 [==============================] - 0s 100us/step - loss: 0.5730 - accuracy: 0.9524\n",
      "Epoch 40/100\n",
      "21/21 [==============================] - 0s 74us/step - loss: 0.5694 - accuracy: 0.9524\n",
      "Epoch 41/100\n",
      "21/21 [==============================] - 0s 94us/step - loss: 0.5658 - accuracy: 0.9524\n",
      "Epoch 42/100\n",
      "21/21 [==============================] - 0s 89us/step - loss: 0.5622 - accuracy: 0.9524\n",
      "Epoch 43/100\n",
      "21/21 [==============================] - 0s 109us/step - loss: 0.5585 - accuracy: 0.9524\n",
      "Epoch 44/100\n",
      "21/21 [==============================] - 0s 77us/step - loss: 0.5548 - accuracy: 0.9524\n",
      "Epoch 45/100\n",
      "21/21 [==============================] - 0s 70us/step - loss: 0.5511 - accuracy: 0.9524\n",
      "Epoch 46/100\n",
      "21/21 [==============================] - 0s 71us/step - loss: 0.5473 - accuracy: 0.9524\n",
      "Epoch 47/100\n",
      "21/21 [==============================] - 0s 117us/step - loss: 0.5436 - accuracy: 0.9524\n",
      "Epoch 48/100\n",
      "21/21 [==============================] - 0s 80us/step - loss: 0.5397 - accuracy: 0.9524\n",
      "Epoch 49/100\n",
      "21/21 [==============================] - 0s 76us/step - loss: 0.5359 - accuracy: 0.9524\n",
      "Epoch 50/100\n",
      "21/21 [==============================] - 0s 74us/step - loss: 0.5321 - accuracy: 0.9524\n",
      "Epoch 51/100\n",
      "21/21 [==============================] - 0s 134us/step - loss: 0.5282 - accuracy: 0.9524\n",
      "Epoch 52/100\n",
      "21/21 [==============================] - 0s 82us/step - loss: 0.5243 - accuracy: 0.9524\n",
      "Epoch 53/100\n",
      "21/21 [==============================] - 0s 115us/step - loss: 0.5204 - accuracy: 0.9524\n",
      "Epoch 54/100\n",
      "21/21 [==============================] - 0s 90us/step - loss: 0.5164 - accuracy: 0.9524\n",
      "Epoch 55/100\n",
      "21/21 [==============================] - 0s 74us/step - loss: 0.5125 - accuracy: 0.9683\n",
      "Epoch 56/100\n",
      "21/21 [==============================] - 0s 91us/step - loss: 0.5085 - accuracy: 0.9683\n",
      "Epoch 57/100\n",
      "21/21 [==============================] - 0s 70us/step - loss: 0.5045 - accuracy: 0.9683\n",
      "Epoch 58/100\n",
      "21/21 [==============================] - 0s 72us/step - loss: 0.5005 - accuracy: 0.9683\n",
      "Epoch 59/100\n",
      "21/21 [==============================] - 0s 127us/step - loss: 0.4964 - accuracy: 0.9683\n",
      "Epoch 60/100\n",
      "21/21 [==============================] - 0s 86us/step - loss: 0.4924 - accuracy: 0.9683\n",
      "Epoch 61/100\n",
      "21/21 [==============================] - 0s 101us/step - loss: 0.4883 - accuracy: 0.9683\n",
      "Epoch 62/100\n",
      "21/21 [==============================] - 0s 75us/step - loss: 0.4842 - accuracy: 0.9683\n",
      "Epoch 63/100\n",
      "21/21 [==============================] - 0s 76us/step - loss: 0.4802 - accuracy: 0.9683\n",
      "Epoch 64/100\n",
      "21/21 [==============================] - 0s 107us/step - loss: 0.4761 - accuracy: 0.9683\n",
      "Epoch 65/100\n",
      "21/21 [==============================] - 0s 103us/step - loss: 0.4720 - accuracy: 0.9683\n",
      "Epoch 66/100\n",
      "21/21 [==============================] - 0s 100us/step - loss: 0.4678 - accuracy: 0.9683\n",
      "Epoch 67/100\n",
      "21/21 [==============================] - 0s 86us/step - loss: 0.4637 - accuracy: 0.9683\n",
      "Epoch 68/100\n",
      "21/21 [==============================] - 0s 83us/step - loss: 0.4596 - accuracy: 0.9683\n",
      "Epoch 69/100\n",
      "21/21 [==============================] - 0s 95us/step - loss: 0.4555 - accuracy: 0.9683\n",
      "Epoch 70/100\n",
      "21/21 [==============================] - 0s 93us/step - loss: 0.4513 - accuracy: 0.9683\n",
      "Epoch 71/100\n",
      "21/21 [==============================] - 0s 91us/step - loss: 0.4472 - accuracy: 0.9683\n",
      "Epoch 72/100\n",
      "21/21 [==============================] - 0s 86us/step - loss: 0.4431 - accuracy: 0.9683\n",
      "Epoch 73/100\n",
      "21/21 [==============================] - 0s 78us/step - loss: 0.4389 - accuracy: 0.9683\n",
      "Epoch 74/100\n",
      "21/21 [==============================] - 0s 105us/step - loss: 0.4348 - accuracy: 0.9683\n",
      "Epoch 75/100\n",
      "21/21 [==============================] - 0s 61us/step - loss: 0.4307 - accuracy: 0.9683\n",
      "Epoch 76/100\n",
      "21/21 [==============================] - 0s 73us/step - loss: 0.4265 - accuracy: 0.9683\n",
      "Epoch 77/100\n",
      "21/21 [==============================] - 0s 90us/step - loss: 0.4224 - accuracy: 0.9683\n",
      "Epoch 78/100\n",
      "21/21 [==============================] - 0s 77us/step - loss: 0.4183 - accuracy: 0.9683\n",
      "Epoch 79/100\n",
      "21/21 [==============================] - 0s 68us/step - loss: 0.4142 - accuracy: 0.9683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "21/21 [==============================] - 0s 77us/step - loss: 0.4101 - accuracy: 0.9683\n",
      "Epoch 81/100\n",
      "21/21 [==============================] - 0s 74us/step - loss: 0.4060 - accuracy: 0.9683\n",
      "Epoch 82/100\n",
      "21/21 [==============================] - 0s 65us/step - loss: 0.4019 - accuracy: 0.9683\n",
      "Epoch 83/100\n",
      "21/21 [==============================] - 0s 90us/step - loss: 0.3978 - accuracy: 0.9683\n",
      "Epoch 84/100\n",
      "21/21 [==============================] - 0s 68us/step - loss: 0.3937 - accuracy: 0.9683\n",
      "Epoch 85/100\n",
      "21/21 [==============================] - 0s 86us/step - loss: 0.3897 - accuracy: 0.9683\n",
      "Epoch 86/100\n",
      "21/21 [==============================] - 0s 84us/step - loss: 0.3856 - accuracy: 0.9683\n",
      "Epoch 87/100\n",
      "21/21 [==============================] - 0s 94us/step - loss: 0.3816 - accuracy: 0.9683\n",
      "Epoch 88/100\n",
      "21/21 [==============================] - 0s 74us/step - loss: 0.3776 - accuracy: 0.9683\n",
      "Epoch 89/100\n",
      "21/21 [==============================] - 0s 75us/step - loss: 0.3736 - accuracy: 0.9683\n",
      "Epoch 90/100\n",
      "21/21 [==============================] - 0s 66us/step - loss: 0.3696 - accuracy: 0.9683\n",
      "Epoch 91/100\n",
      "21/21 [==============================] - 0s 60us/step - loss: 0.3656 - accuracy: 0.9683\n",
      "Epoch 92/100\n",
      "21/21 [==============================] - 0s 83us/step - loss: 0.3616 - accuracy: 0.9683\n",
      "Epoch 93/100\n",
      "21/21 [==============================] - 0s 87us/step - loss: 0.3577 - accuracy: 0.9683\n",
      "Epoch 94/100\n",
      "21/21 [==============================] - 0s 71us/step - loss: 0.3538 - accuracy: 0.9683\n",
      "Epoch 95/100\n",
      "21/21 [==============================] - 0s 59us/step - loss: 0.3499 - accuracy: 0.9683\n",
      "Epoch 96/100\n",
      "21/21 [==============================] - 0s 71us/step - loss: 0.3460 - accuracy: 0.9683\n",
      "Epoch 97/100\n",
      "21/21 [==============================] - 0s 74us/step - loss: 0.3422 - accuracy: 0.9683\n",
      "Epoch 98/100\n",
      "21/21 [==============================] - 0s 95us/step - loss: 0.3383 - accuracy: 0.9683\n",
      "Epoch 99/100\n",
      "21/21 [==============================] - 0s 79us/step - loss: 0.3345 - accuracy: 0.9683\n",
      "Epoch 100/100\n",
      "21/21 [==============================] - 0s 73us/step - loss: 0.3307 - accuracy: 0.9683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13b991e80>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_seq,y,epochs=100,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 6, 42, 43], [20, 3], [18, 2, 19, 20, 3], [11, 11, 3]]\n",
      "[[0.70264834 0.22311184 0.2455397 ]\n",
      " [0.46696186 0.2636264  0.3039347 ]\n",
      " [0.20981568 0.71370476 0.21754959]\n",
      " [0.37569633 0.23620367 0.5102246 ]]\n",
      "[0.70264834 0.22311184 0.2455397 ]\n",
      "0\n",
      "[0.46696186 0.2636264  0.3039347 ]\n",
      "0\n",
      "[0.20981568 0.71370476 0.21754959]\n",
      "1\n",
      "[0.37569633 0.23620367 0.5102246 ]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#prediction \n",
    "instance = [\"It was very boring\",'excellent movie','This is an excellent movie','ok ok movie']\n",
    "instance = word_tokenizer.texts_to_sequences(instance)\n",
    "print(instance)\n",
    "instance = pad_sequences(instance, padding='post', maxlen=longest_seq_length)\n",
    "y_pred = model.predict(instance)\n",
    "print(y_pred)\n",
    "for x in y_pred:\n",
    "    print(x)\n",
    "    print(np.argmax(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now Let's extend above example and test with actual reuters  dataset\n",
    "dataset = keras.datasets.reuters.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982,) (8982,) (2246,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexes = keras.datasets.reuters.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {}\n",
    "for key,val in word_indexes.items():\n",
    "    index_to_word[val] = key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line(index):\n",
    "    return ' '.join([index_to_word[word] for word in x_train[index]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the wattie nondiscriminatory mln loss for plc said at only ended said commonwealth could 1 traders now april 0 a after said from 1985 and from foreign 000 april 0 prices its account year a but in this mln home an states earlier and rise and revs vs 000 its 16 vs 000 a but 3 psbr oils several and shareholders and dividend vs 000 its all 4 vs 000 1 mln agreed largely april 0 are 2 states will billion total and against 000 pct dlrs 3\n",
      "the termination payment airport takes 6 visibility geological 3 6 602 begin up said fully bank expects commodity total is giant a recreation this takes leroy series termination payment airport mln a for capital 1 pre 50 american east said in council takes leroy recommend's france a but u any 4 s 1st losses pct dlrs 4\n",
      "the lt dlrs demand 000 reuter dividend year lt plus billion 04 000 reuter dividend year an worth new vs reuter dlrs warburg on shrs earnings countries new vs reuter 1985 billion vs 2 lt 4 division 000 reuter from go 000 lt plus which mid 000 reuter from total 000 an 71 billion vs reuter dlr also vs shrs earnings countries 4 vs reuter 1985 from vs some now april 0 related in corp it inc strong cents dollar were after april 0 crisis or ontario more index 10 electric company taking report it in estimated but trading texas said united said came a advising up said countries vs 000 3 delayed central said which objections on future 617 said came a includes refile profit said meeting trade vs 3 supplie up said 1985 were vs pct dlrs 3\n"
     ]
    }
   ],
   "source": [
    "print(get_line(0),y_train[0])\n",
    "print(get_line(1),y_train[1])\n",
    "print(get_line(2),y_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 37, 38, 309, 213, 170, 4, 480, 373, 287, 3519, 13, 4, 48, 193, 6157, 7, 11, 1675, 31, 4034, 1115, 506, 399, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 1302, 11, 1148, 1624, 957, 61, 957, 61, 928, 47, 928, 47, 2742, 929, 32, 929, 32, 388, 32, 388, 32, 1146, 4594, 2038, 12, 2038, 12, 2098, 70, 2098, 70, 373, 11, 4594, 624, 80, 47, 3958, 47, 3958, 19, 5614, 19, 5614, 122, 63, 3658, 63, 3658, 63, 5201, 63, 5201, 135, 1717, 134, 2885, 134, 2885, 83, 3703, 83, 3703, 1717, 235, 203, 399, 6976, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 2781, 1187, 47, 315, 47, 445, 47, 6370, 47, 2012, 100, 19, 1684, 19, 649, 19, 1527, 19, 2084, 4394, 11312, 44, 4443, 44, 1663, 44, 2860, 44, 2860, 162, 19, 2304, 19, 999, 19, 3455, 19, 3455, 135, 489, 61, 5156, 59, 677, 59, 5246, 59, 5246, 153, 413, 44, 1891, 44, 6184, 47, 3958, 47, 3958, 4862, 14645, 19, 628, 19, 445, 4670, 4670, 1641, 413, 19, 677, 19, 315, 4709, 4709, 664, 413, 32, 1193, 32, 8364, 32, 3870, 32, 3870, 10282, 19, 352, 19, 566, 19, 352, 19, 566, 32, 321, 32, 321, 123, 96, 7, 15, 90, 4594, 399, 760, 2698, 1809, 19, 137, 229, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 1302, 11, 1148, 1624, 785, 70, 785, 70, 388, 59, 388, 59, 2742, 376, 61, 376, 61, 750, 61, 750, 61, 1146, 492, 47, 492, 47, 539, 44, 539, 44, 373, 11, 4594, 624, 3690, 19, 5439, 19, 5439, 19, 2713, 19, 2713, 122, 32, 8015, 32, 8015, 32, 2713, 32, 2713, 135, 373, 1717, 47, 6728, 47, 6728, 12, 5679, 12, 5679, 1717, 235, 203, 137, 229, 48, 193, 105, 462, 6976, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 2781, 675, 677, 5650, 4678, 4678, 4911, 1052, 512, 1038, 1038, 1187, 1273, 2845, 3403, 3403, 4394, 11312, 19, 2653, 19, 1668, 19, 4371, 19, 4371, 162, 19, 4737, 19, 4737, 5302, 5302, 135, 489, 32, 2873, 32, 1749, 19, 3548, 19, 3548, 153, 413, 19, 6675, 19, 5201, 19, 5439, 19, 5439, 4862, 14645, 4073, 1663, 4215, 4215, 1641, 413, 2819, 2819, 3909, 3909, 664, 413, 6292, 5204, 4456, 4456, 180, 96, 32, 124, 279, 32, 124, 279, 12, 1588, 12, 1588, 123, 96, 7, 15, 90, 2388, 229, 760, 2698, 252, 19, 1116, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 1302, 11, 1148, 1624, 860, 44, 860, 44, 924, 19, 860, 19, 2742, 981, 47, 981, 47, 860, 59, 860, 59, 1146, 4594, 502, 63, 502, 63, 492, 19, 492, 19, 373, 11, 4594, 624, 413, 3703, 3703, 3411, 3411, 122, 32, 6728, 32, 6728, 32, 5890, 32, 5890, 135, 32, 3253, 32, 3253, 32, 5476, 32, 5476, 1116, 6976, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 2781, 9559, 19, 1749, 19, 2121, 19, 5356, 19, 5356, 162, 677, 677, 4026, 4026, 4911, 1187, 9, 3640, 1734, 1038, 462, 462, 135, 489, 19, 4927, 19, 6334, 19, 5042, 19, 5042, 153, 413, 3978, 4350, 3703, 3703, 180, 96, 47, 376, 47, 401, 47, 376, 47, 401, 44, 1090, 44, 1090, 123, 305, 96, 7, 15, 90, 2388, 709, 760, 2698, 252, 19, 2625, 1717, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 1302, 11, 1148, 1624, 2038, 63, 2038, 63, 2611, 19, 2611, 19, 2742, 1745, 70, 1745, 70, 2005, 63, 2005, 63, 28170, 82, 32, 722, 32, 722, 32, 425, 32, 425, 373, 11, 82, 624, 413, 2012, 47, 2012, 47, 841, 44, 841, 44, 122, 3821, 47, 3821, 47, 3403, 47, 3403, 47, 203, 70, 59, 70, 59, 70, 72, 70, 72, 135, 4656, 47, 4656, 47, 3661, 61, 3661, 61, 1717, 235, 399, 1780, 1425, 3527, 2625, 1717, 6976, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 2781, 1187, 1193, 59, 2455, 32, 2653, 63, 1870, 44, 100, 352, 63, 352, 70, 352, 70, 492, 12, 4394, 11312, 3259, 47, 3090, 32, 2219, 63, 2219, 63, 162, 688, 72, 279, 63, 597, 59, 597, 59, 135, 489, 2425, 12, 2839, 72, 3423, 47, 3423, 47, 153, 413, 2873, 19, 3072, 44, 2012, 47, 2012, 47, 4862, 14645, 749, 70, 597, 44, 158, 59, 158, 59, 1641, 413, 579, 32, 648, 44, 119, 47, 119, 47, 664, 413, 750, 63, 1063, 44, 1012, 12, 1012, 12, 1717, 235, 399, 1780, 3527, 1425, 6157, 13, 3527, 1425, 792, 252, 19, 399, 9, 1780, 1809, 19, 709, 65, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 373, 11, 1601, 624, 3690, 4575, 4575, 5587, 5587, 122, 134, 4994, 134, 1734, 160, 4517, 160, 4517, 203, 319, 319, 63, 63, 135, 165, 2116, 165, 3819, 134, 2817, 134, 2817, 123, 105, 462, 122, 828, 267, 21, 540, 29, 5877, 5, 19, 7928, 11, 1675, 709, 65, 6976, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 2781, 11, 1601, 260, 83, 315, 83, 315, 83, 5356, 83, 5356, 162, 19, 1273, 19, 1273, 19, 2817, 19, 2817, 135, 160, 2228, 160, 2228, 160, 2941, 160, 2941, 153, 3690, 19, 2013, 19, 451, 4575, 4575, 10282, 187, 44, 158, 70, 102, 70, 191, 70, 196, 557, 196, 557, 123, 305, 96, 7, 514, 90, 1576, 760, 13, 709, 65, 2698, 2688, 19, 709, 4714, 2019, 7, 4717, 434, 835, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 624, 3690, 3232, 3232, 3611, 3611, 122, 365, 6079, 365, 3296, 296, 4161, 296, 4161, 135, 365, 4921, 365, 5476, 147, 4460, 147, 4460, 123, 105, 462, 122, 828, 267, 21, 540, 29, 5877, 5, 19, 7928, 11, 1675, 9719, 4714, 2019, 6976, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 2781, 27420, 434, 835, 260, 119, 18, 258, 1442, 258, 5891, 258, 2098, 162, 59, 315, 59, 1273, 59, 5942, 59, 4969, 135, 365, 315, 365, 251, 147, 2012, 147, 2012, 153, 3690, 2566, 3412, 3232, 3232, 10282, 1439, 649, 1439, 649, 2447, 512, 2447, 512, 123, 96, 7, 15, 90, 434, 1307, 760, 13, 709, 4714, 9, 2019, 2698, 2688, 19, 877, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 654, 11, 1148, 1624, 83, 1424, 83, 1424, 83, 844, 83, 844, 2742, 63, 648, 63, 648, 83, 321, 83, 321, 1146, 1601, 5553, 4906, 4153, 4153, 373, 11, 1922, 1576, 2165, 624, 11982, 1717, 72, 352, 72, 352, 47, 83, 47, 83, 122, 72, 482, 72, 1036, 165, 688, 165, 688, 4394, 373, 2486, 258, 1424, 258, 187, 191, 841, 191, 841, 1717, 267, 21, 7775, 1402, 563, 2486, 235, 203, 877, 6976, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 2781, 260, 61, 83, 61, 1735, 59, 279, 59, 279, 162, 59, 865, 59, 957, 19, 977, 19, 977, 135, 165, 957, 165, 948, 63, 597, 63, 597, 153, 413, 44, 279, 44, 648, 72, 352, 72, 352, 10775, 96, 533, 61, 1717, 533, 61, 1717, 758, 132, 758, 132, 1717, 48, 193, 96, 23, 3220, 305, 13, 99, 142, 177, 5, 667, 29, 45, 10, 5692, 13, 48, 193, 305, 96, 7, 514, 90, 1576, 877, 760, 2698, 748, 19, 953, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 1302, 11, 1148, 1624, 32, 279, 32, 279, 32, 533, 32, 533, 2742, 32, 546, 32, 546, 32, 648, 32, 648, 1146, 1601, 44, 5614, 44, 5614, 44, 5475, 44, 5475, 373, 11, 9417, 624, 3690, 948, 12, 948, 12, 750, 61, 750, 61, 122, 2653, 47, 2653, 47, 2653, 72, 2653, 72, 203, 32, 32, 32, 32, 32, 32, 32, 32, 135, 3118, 72, 3118, 72, 3236, 63, 3236, 63, 953, 6976, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 2781, 11, 9417, 260, 934, 70, 934, 70, 566, 63, 566, 63, 162, 401, 70, 401, 70, 746, 61, 746, 61, 135, 2486, 1899, 70, 1899, 70, 2154, 44, 2154, 44, 153, 413, 865, 72, 865, 72, 948, 12, 948, 12, 1641, 413, 614, 72, 614, 72, 704, 44, 704, 44, 664, 413, 296, 70, 296, 70, 352, 63, 352, 63, 10282, 12, 425, 47, 147, 12, 425, 47, 147, 59, 745, 59, 745, 123, 305, 96, 7, 15, 90, 7089, 2486, 10802, 1279, 651, 10, 45, 554, 634, 6528, 96, 5513, 208, 6, 667, 377, 953, 760, 2698, 748, 19, 1780, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 1146, 4594, 934, 61, 934, 61, 865, 63, 865, 63, 373, 11, 4594, 624, 3690, 4271, 4271, 445, 445, 122, 7007, 7007, 19, 1050, 19, 1050, 135, 19, 5247, 19, 5247, 19, 2712, 19, 2712, 2781, 11, 4594, 1187, 2417, 4598, 5775, 5775, 100, 124, 124, 440, 440, 4394, 11312, 3525, 4320, 6366, 6366, 1780, 6976, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 162, 2140, 2140, 2859, 2859, 135, 489, 5329, 4423, 5809, 5809, 153, 413, 4795, 6798, 4271, 4271, 10775, 96, 19, 124, 132, 19, 124, 132, 19, 1038, 19, 1038, 123, 96, 7, 15, 90, 2388, 1780, 760, 2698, 1809, 19, 1425, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 1146, 4594, 132, 63, 132, 63, 533, 70, 533, 70, 624, 413, 2845, 2845, 4074, 4074, 122, 3978, 3978, 4835, 4835, 203, 44, 44, 72, 72, 135, 4326, 4326, 5438, 5438, 1425, 6976, 48, 193, 105, 462, 821, 589, 193, 831, 102, 193, 821, 589, 193, 831, 102, 193, 2781, 11, 4594, 1187, 445, 445, 3429, 3429, 100, 2251, 2251, 3072, 3072, 4394, 11312, 4073, 4073, 315, 315, 162, 649, 649, 272, 272, 135, 489, 3121, 3121, 4619, 4619, 153, 413, 3411, 3411, 2845, 2845, 10282, 19, 425, 566, 19, 425, 566, 19, 1019, 19, 1019, 123, 305, 96, 7, 15, 90, 2388, 1425, 760, 2698, 252, 19, 3527, 7, 11, 1675, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 1146, 4594, 758, 70, 758, 70, 924, 61, 924, 61, 624, 3690, 3229, 3229, 1684, 1684, 122, 3768, 3768, 4482, 4482, 203, 124, 124, 284, 284, 135, 4016, 4016, 5672, 5672, 3527, 7, 11, 1675, 6976, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 2781, 1187, 628, 628, 3125, 3125, 100, 635, 635, 928, 928, 4394, 4731, 4032, 4032, 3253, 3253, 162, 32, 32, 32, 32, 135, 3914, 3914, 3920, 3920, 153, 3690, 2005, 2005, 3229, 3229, 10282, 19, 557, 119, 19, 557, 119, 19, 321, 19, 321, 123, 305, 96, 7, 15, 90, 2388, 3527, 760, 2698, 252, 19, 295, 361, 953, 7, 11, 9417, 251, 1601, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 2742, 1148, 11, 19, 928, 19, 928, 19, 1013, 19, 1013, 1146, 1601, 44, 4072, 44, 4072, 44, 2543, 44, 2543, 624, 11982, 648, 12, 648, 12, 539, 61, 539, 61, 122, 1138, 63, 1138, 63, 251, 47, 251, 47, 4394, 373, 1967, 59, 1967, 59, 1193, 19, 1193, 19, 123, 1505, 413, 535, 45, 439, 3436, 7147, 373, 2677, 489, 535, 45, 1363, 798, 413, 7, 10042, 135, 373, 235, 203, 51, 45, 3436, 7147, 295, 361, 953, 7, 11, 9417, 251, 1601, 6976, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 260, 489, 688, 70, 688, 70, 722, 63, 722, 63, 162, 566, 70, 376, 70, 614, 70, 614, 70, 135, 489, 1929, 70, 1734, 70, 512, 63, 512, 63, 153, 413, 1717, 279, 59, 425, 59, 648, 12, 648, 12, 17448, 12, 425, 47, 147, 12, 425, 47, 296, 59, 462, 59, 462, 123, 305, 96, 7, 15, 90, 7089, 1717, 3436, 7147, 45, 777, 373, 2677, 489, 535, 45, 1363, 798, 413, 7, 10042, 953, 760, 2698, 748, 19, 1430, 434, 361, 953, 7, 11, 9417, 251, 1601, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 2742, 1148, 11, 70, 579, 70, 579, 70, 579, 70, 579, 1146, 1601, 59, 5350, 59, 5350, 59, 3499, 59, 3499, 624, 11982, 365, 61, 365, 61, 147, 61, 147, 61, 122, 597, 59, 597, 59, 492, 44, 492, 44, 4394, 373, 566, 12, 566, 12, 860, 61, 860, 61, 123, 1505, 413, 535, 45, 439, 3436, 7147, 373, 2677, 489, 535, 45, 1363, 798, 413, 7, 10042, 135, 373, 235, 203, 51, 45, 3436, 7147, 1430, 434, 361, 953, 7, 11, 9417, 251, 1601, 6976, 48, 193, 105, 462, 821, 589, 193, 831, 589, 193, 821, 589, 193, 831, 589, 193, 260, 489, 296, 70, 296, 70, 191, 70, 191, 70, 162, 102, 70, 119, 70, 158, 61, 158, 61, 135, 489, 749, 70, 653, 70, 502, 61, 502, 61, 153, 413, 1717, 296, 44, 258, 44, 365, 61, 365, 61, 17448, 12, 425, 47, 147, 12, 425, 47, 147, 44, 1045, 44, 1045, 123, 305, 96, 7, 15, 90, 7089, 1717, 3436, 7147, 45, 777, 373, 2677, 489, 535, 45, 1363, 798, 413, 7, 10042, 953, 760, 2698, 748, 19, 975, 21, 37, 38, 373, 287, 15624, 651, 10, 45, 554, 6604, 91, 45, 1144, 208, 6, 8950, 285, 13, 48, 193, 55, 11295, 5, 634, 715, 1187, 2781, 13, 399, 229, 709, 2625, 1780, 1425, 3527, 235, 3640, 513, 3640, 513, 777, 7, 953, 9, 1430, 434, 361, 953, 260, 2781, 953, 295, 361, 9, 1430, 434, 361, 953, 305, 96, 13, 105, 462, 828, 9, 48, 193, 3519, 55, 76, 87, 9, 1532, 197, 2503, 136, 4, 667, 377, 224, 93, 48, 17, 12] 2376\n"
     ]
    }
   ],
   "source": [
    "word_count = lambda sentence : len(sentence) \n",
    "longest_seq = max(x_train,key=word_count) \n",
    "longest_seq_length = len(longest_seq)\n",
    "print(longest_seq,longest_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1 27595 28842 ...     0     0     0]\n",
      " [    1  3267   699 ...     0     0     0]\n",
      " [   19   758    15 ...    11    17    12]\n",
      " ...\n",
      " [    1   141  3890 ...     0     0     0]\n",
      " [   88  2321    72 ...   364    17    12]\n",
      " [  125  2855    21 ...   113    17    12]]\n"
     ]
    }
   ],
   "source": [
    "longest_seq_length=100#keeping max longest seq length to 100\n",
    "padded_seq = keras.preprocessing.sequence.pad_sequences(x_train,longest_seq_length,padding=\"post\")\n",
    "print(padded_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30979\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(index_to_word)+1\n",
    "\n",
    "print(len(index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_padded = keras.preprocessing.sequence.pad_sequences(x_test,longest_seq_length,padding=\"post\")\n",
    "y_train_cat = keras.utils.to_categorical(y_train)\n",
    "y_test_cat = keras.utils.to_categorical(y_test)\n",
    "y_train_cat.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 100)          3098000   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 46)                460046    \n",
      "=================================================================\n",
      "Total params: 3,558,046\n",
      "Trainable params: 3,558,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_length,100,input_length=longest_seq_length))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(y_train_cat.shape[1],activation='sigmoid'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30979"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(padded_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/10\n",
      "8982/8982 [==============================] - 1s 107us/step - loss: 0.0812 - accuracy: 0.9810 - val_loss: 0.0714 - val_accuracy: 0.9816\n",
      "Epoch 2/10\n",
      "8982/8982 [==============================] - 1s 110us/step - loss: 0.0685 - accuracy: 0.9817 - val_loss: 0.0667 - val_accuracy: 0.9826\n",
      "Epoch 3/10\n",
      "8982/8982 [==============================] - 1s 108us/step - loss: 0.0633 - accuracy: 0.9829 - val_loss: 0.0626 - val_accuracy: 0.9836\n",
      "Epoch 4/10\n",
      "8982/8982 [==============================] - 1s 113us/step - loss: 0.0593 - accuracy: 0.9837 - val_loss: 0.0599 - val_accuracy: 0.9843\n",
      "Epoch 5/10\n",
      "8982/8982 [==============================] - 1s 109us/step - loss: 0.0563 - accuracy: 0.9850 - val_loss: 0.0580 - val_accuracy: 0.9854\n",
      "Epoch 6/10\n",
      "8982/8982 [==============================] - 1s 111us/step - loss: 0.0537 - accuracy: 0.9860 - val_loss: 0.0562 - val_accuracy: 0.9860\n",
      "Epoch 7/10\n",
      "8982/8982 [==============================] - 1s 107us/step - loss: 0.0509 - accuracy: 0.9868 - val_loss: 0.0543 - val_accuracy: 0.9866\n",
      "Epoch 8/10\n",
      "8982/8982 [==============================] - 1s 123us/step - loss: 0.0481 - accuracy: 0.9877 - val_loss: 0.0525 - val_accuracy: 0.9871\n",
      "Epoch 9/10\n",
      "8982/8982 [==============================] - 1s 115us/step - loss: 0.0453 - accuracy: 0.9883 - val_loss: 0.0508 - val_accuracy: 0.9874\n",
      "Epoch 10/10\n",
      "8982/8982 [==============================] - 1s 114us/step - loss: 0.0426 - accuracy: 0.9887 - val_loss: 0.0492 - val_accuracy: 0.9878\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x142991668>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_seq,y_train_cat,epochs=10,verbose=1,batch_size=500,shuffle=False,validation_data=(x_test_padded,y_test_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2246/2246 [==============================] - 0s 38us/step\n",
      "[0.04922867750123687, 0.9877753853797913]\n"
     ]
    }
   ],
   "source": [
    "#Let's predict and eval\n",
    "test_score = model.evaluate(x_test_padded,y_test_cat)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test = 3\n",
      "y_pred = 3\n",
      "y_test = 10\n",
      "y_pred = 19\n",
      "y_test = 1\n",
      "y_pred = 16\n",
      "y_test = 4\n",
      "y_pred = 4\n",
      "y_test = 4\n",
      "y_pred = 4\n",
      "y_test = 3\n",
      "y_pred = 3\n",
      "y_test = 3\n",
      "y_pred = 3\n",
      "y_test = 3\n",
      "y_pred = 3\n",
      "y_test = 3\n",
      "y_pred = 3\n",
      "y_test = 3\n",
      "y_pred = 3\n"
     ]
    }
   ],
   "source": [
    "def print_data(index,x_test,y_test,model):\n",
    "    current_test_data = [x_test[index]]\n",
    "    line = ' '.join([index_to_word[word] for word in current_test_data[0]])\n",
    "    #print(line)\n",
    "    print('y_test = '  + str(y_test[index]))\n",
    "    current_test_data_padded = keras.preprocessing.sequence.pad_sequences(current_test_data,longest_seq_length,padding=\"post\")\n",
    "    y_test_cat = [keras.utils.to_categorical(y_test[index])]\n",
    "    y_pred = model.predict(current_test_data_padded)\n",
    "    #print(y_pred)\n",
    "    print('y_pred = ' + str(np.argmax(y_pred)))\n",
    "    return \n",
    "\n",
    "\n",
    "#keras.utils.to_categorical(y_test[1])\n",
    "for i in range(10):\n",
    "    print_data(i,x_test,y_test,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
